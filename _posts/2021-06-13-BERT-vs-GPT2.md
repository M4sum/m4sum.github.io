---
layout: post
title: "GPT2 vs BERT"
---

## Table of contents
- [Introduction](#introduction)
- [Datasets](#datasets)
- [Analysis](#analysis)
- [Conclusion](#conclusion)


# [Introduction](#introduction)

The aim of this project is to analyze and compare the predictive performance of BERT and
GPT-2 models on different datasets to help improve the understanding of these two
architectures.

We code our own masked language modeling task:
1. Randomly collect 10,000 sample sequences from a dataset
2. For each sample, randomly mask a token
3. Predict the masked word

We perform this task on two datasets, Wikitext and Ag_news, with pretrained BERT and GPT2
models. For BERT, this is a masked language modeling task. BERT uses bidirectional language
representations so it can predict the masked word given the context on both sides. For GPT2,
this is an autoregressive language modeling task. GPT2 only uses the preceding sequence of
words as context.

This report introduces the task, the datasets used, the models used, and includes graphs of the
results.